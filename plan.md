# Revised Technical Design: Ozone Recon Chatbot (Phase 1 with Gemini)

This document outlines the revised technical design for an intelligent chatbot for Ozone Recon, built from the ground up using the Gemini language model. This approach replaces the previous keyword-based system with a dynamic, LLM-powered agent.

## 1. Architecture and Data Flow

The new architecture centers around a **Gemini-powered agent** that uses the Recon API specification as its "toolset." Instead of hardcoded rules, the agent will reason over the API documentation to answer user queries.

The data flow will be a multi-step process:

```
User Query -> [Agent orchestrates with Gemini]
    1. Gemini receives Query + API Schema -> determines which API to call.
    2. Agent executes the API call -> gets back raw JSON data.
    3. Gemini receives JSON data + original Query -> generates a natural language summary.
-> Formatted Response
```

**Breakdown of the Flow:**

1.  **User Query**: The user asks a question in natural language (e.g., "how many open keys are there, and what's their total size?").
2.  **Context Assembly**: The application constructs a prompt for Gemini that includes:
    *   A **system prompt** instructing Gemini to act as an expert on Apache Ozone Recon.
    *   The **full contents of the `recon-api.yaml` file**, which serves as the "documentation" or "toolset" for the model.
    *   The user's query.
3.  **First Gemini Call (Intent-to-Tool)**: The application sends the prompt to Gemini. Using its reasoning capabilities, Gemini analyzes the query against the provided API schema and determines the most appropriate API endpoint to call. Its response will be a structured "tool call" (e.g., `call_recon_api(endpoint='/keys/open/summary')`).
4.  **API Client Execution**: The application parses the tool call from Gemini and uses a standard HTTP client to make a request to the specified Recon API endpoint.
5.  **Second Gemini Call (Summarization)**: The raw JSON response from the API is received. The application then makes a *second* call to Gemini with a new prompt, such as: *"The user asked: 'how many open keys are there...'. The API returned the following data: `{...JSON data...}`. Please summarize this data to answer the user's question."*
6.  **Formatted Response**: Gemini generates a human-readable summary (e.g., "There are 152 open keys, consuming a total replicated size of 1.2 GB."), which is then returned to the user.

## 2. Core Components

The previous design's `Intent Classifier` and `intent_mapping.yaml` are no longer needed. They are replaced by:

*   **Gemini Client**: A dedicated module to handle all communications with the Gemini API. It will manage the API key, format prompts, and parse responses (both tool calls and final summaries).
*   **Chatbot Agent**: The central orchestrator. This module manages the multi-step conversation flow described above—sending the initial prompt, executing the tool call, and sending the data back for summarization.
*   **API Schema (`recon-api.yaml`)**: This file is promoted from a simple reference to a core asset that is loaded into the Gemini context window at the start of every conversation.

## 3. Fallback Logic

Fallback handling becomes a natural capability of the LLM. If a user asks a question that cannot be answered by the endpoints in `recon-api.yaml` (e.g., "what's the weather like?"), Gemini will reason that none of its available tools are suitable and generate a helpful response like, "I'm sorry, I can only answer questions about the Ozone Recon cluster. My capabilities are limited to the information available through its REST API."

## 4. Proposed Code Structure

The project structure is now simpler and more focused on the agent's logic.

```
recon-chatbot/
├── main.py                 # Main application entry point
├── chatbot_agent.py        # Core agent logic for orchestrating the conversation
├── gemini_client.py        # Handles all communication with the Gemini API
├── api_client.py           # (Unchanged) Handles HTTP requests to the Recon API
└── api_schema/
    └── recon-api.yaml      # The API specification, loaded as context for the LLM
```

## 5. Implementation Steps

### Phase 1 Tasks:
- [ ] Set up the initial project structure with placeholder files for the Gemini-based agent
- [ ] Create an `api_schema/` directory and place the `recon-api.yaml` file inside it
- [ ] Implement the `GeminiClient` to handle authentication, prompt construction, and communication with the Gemini API
- [ ] Implement the `ApiClient` to execute GET requests based on the tool calls generated by Gemini
- [ ] Implement the core logic in `ChatbotAgent` to orchestrate the multi-step LLM flow: context assembly, tool call, API execution, and summarization
- [ ] Integrate all modules in `main.py` to create the end-to-end user-facing application

## 6. Key Advantages of the Gemini Approach

1. **Dynamic Understanding**: No need to predefine keyword mappings - Gemini can understand varied natural language expressions
2. **Intelligent Parameter Extraction**: Gemini can extract parameters from natural language (e.g., extracting "/vol1/bucket1" from "show me usage for volume1 bucket1")
3. **Contextual Responses**: Responses are generated based on the actual data and user's specific question
4. **Multi-endpoint Reasoning**: Can potentially combine information from multiple API calls in future iterations
5. **Natural Fallbacks**: Intelligent handling of out-of-scope queries

This approach leverages Gemini's 1 million token context window to load the entire API specification, making it a true "expert" on the Recon API capabilities.
